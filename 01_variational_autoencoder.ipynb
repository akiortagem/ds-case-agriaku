{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "828a9b48-c11e-49d9-8f7d-1f167c618ff9",
   "metadata": {},
   "source": [
    "## Case 01: Variational Autoencoder (VAE)\n",
    "* Dataset: CIFAR100\n",
    "* DL Framework: Tensorflow-Keras\n",
    "* DL Task: Image reconstruction\n",
    "\n",
    "`PREREQUISITE` All modules (with their suitable versions) are installed properly.\n",
    "<br>`TASK` Complete the notebook cell's code marked with <b>#TODO</b> comment.\n",
    "<br>`OBJECTIVE` Achieve a min. validation accuracy of <b>90%</b> within <b>10 epochs</b>.\n",
    "<br>`WARNING` Do <b>NOT</b> change any codes in the <i>config.ipynb</i> file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60f08af-127c-49f9-9ccf-38dba84332ca",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b101e209-68f8-4d3b-9a6a-5aa9d05a1091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "from ipynb.fs.full.config import init, load_cifar_100_data, accuracy, show_reconstructions\n",
    "import ssl\n",
    "import math\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2228afae-4fd2-4782-845d-e9597cf8f3e8",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ec8e7ec-e3df-4a03-b8ca-8f077a487421",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 32, 32, 3) (5000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "(X_train, _), (X_valid, _), (_, _) = load_cifar_100_data()\n",
    "print(X_train.shape, X_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8b3b20-da56-4a2e-8d7b-162d0775ef20",
   "metadata": {},
   "source": [
    "### Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9077376c-583e-44c4-b0a5-f667ddf9ab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = init()[\"EPOCH\"] # DO NOT CHANGE THIS\n",
    "SEED = init()[\"SEED\"] # DO NOT CHANGE THIS\n",
    "BATCH_SIZE = 16 #TODO: set hyperparameters (int)\n",
    "CODINGS_SIZE = 16 #TODO: set parameters for the latent space representation (int)\n",
    "LEARNING_RATE = 0.0001 #TODO: set hyperparameters (int)\n",
    "IMG_SIZE = (32,32,3) #TODO: define the image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b07762cd-b5c8-4b6f-834e-a56d9d93ba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_random = tf.random\n",
    "np_random = np.random\n",
    "\n",
    "K = keras.backend\n",
    "tf_random.set_seed(SEED) # for reproduciable results\n",
    "np_random.seed(SEED) # for reproduciable results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7303feb-9954-4199-8ad1-2fde822019a9",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f7868a1-6a37-4794-a15f-8ebe04755573",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_shape = tf.shape\n",
    "class Sampling(keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        mean, log_var = inputs\n",
    "        return K.random_normal(tf_shape(log_var)) * K.exp(log_var / 2) + mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aef3cf7-baa4-419c-b6b0-01030115af59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(i, c, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    function name:\n",
    "        encoder() = to compress the image inputs (latent)\n",
    "    function parameters:\n",
    "        i = image size\n",
    "        c = coding size\n",
    "    \"\"\"\n",
    "    inputs = keras.layers.Input(shape=i)\n",
    "    #TODO: define the layers stack for the encoder part. you may use MLP, LSTM, or CNN.\n",
    "    # variable 'z' will be the first layer\n",
    "    # z = keras.layers.Conv2D(8, 3, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    # z = keras.layers.Conv2D(8, 3, activation='relu', padding='same', kernel_initializer='he_normal')(z)\n",
    "    # z = keras.layers.MaxPooling2D()(z)\n",
    "    # z = keras.layers.Conv2D(256, 3, strides=1, padding='valid')(inputs)\n",
    "    # z = keras.layers.Conv2D(1, 3, strides=1, padding='same')(z)\n",
    "    z = keras.layers.Dense(8)(inputs)\n",
    "    z = keras.layers.Dense(5)(z)\n",
    "    z = keras.layers.Dense(3)(z)\n",
    "    z = keras.layers.Flatten()(z)\n",
    "\n",
    "    codings_mean = keras.layers.Dense(c)(z)\n",
    "    codings_log_var = keras.layers.Dense(c)(z)\n",
    "    codings = Sampling()([codings_mean, codings_log_var])\n",
    "    return keras.models.Model(inputs=[inputs], outputs=[codings_mean, codings_log_var, codings]), inputs, codings_mean, codings_log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f95db548-2ac7-4a6d-aeb0-db9a3e2ab85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(i, c, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    function name:\n",
    "        decoder = to reconstruct the image (with the same size) from the compressed version (latent)\n",
    "    function parameters:\n",
    "        i = image size\n",
    "        c = coding size\n",
    "    \"\"\"\n",
    "    decoder_inputs = keras.layers.Input(shape=[c])\n",
    "    #TODO: define the layers stack for the decoder part. you may use MLP, LSTM, or CNN.\n",
    "    # variable 'x' will be the first layer\n",
    "    embedding = keras.layers.Dense(np.prod((32, 32, 3)), activation='relu')(decoder_inputs)\n",
    "    embedding = keras.layers.Reshape([32, 32, 3])(embedding)\n",
    "    # x = keras.layers.UpSampling2D()(embedding)\n",
    "    # x = keras.layers.Conv2D(48, 3, activation='relu', padding = 'same', kernel_initializer = 'he_normal')(x)\n",
    "    # x = keras.layers.Conv2D(48, 3, activation='relu', padding = 'same', kernel_initializer = 'he_normal')(x)\n",
    "    # x = keras.layers.Conv2DTranspose(256,(3,3), padding='valid', strides=1)(embedding)\n",
    "    # x = keras.layers.Conv2DTranspose(3,(3,3),padding='same', strides=1)(x)\n",
    "    # x = keras.layers.Conv2DTranspose(18,(3,3),padding='valid', strides=1)(x)\n",
    "    # x = keras.layers.Conv2DTranspose(3, (3,3))(x)\n",
    "    x = keras.layers.Dense(8)(embedding)\n",
    "    x = keras.layers.Dense(5)(x)\n",
    "    x = keras.layers.Dense(3)(x)\n",
    "    outputs = keras.layers.Dense(3)(x)\n",
    "    \n",
    "    # outputs = keras.layers.Conv2DTranspose(3, 3)(x)\n",
    "    # outputs = keras.layers.Conv2DTranspose(3, (3,3), padding='same', strides=1)(x)\n",
    "    return keras.models.Model(inputs=[decoder_inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a49b3504-b319-4be0-b7ba-7c290d585367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32, 32, 8)    32          ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 32, 32, 5)    45          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 32, 32, 3)    18          ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 3072)         0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 16)           49168       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 16)           49168       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " sampling (Sampling)            (None, 16)           0           ['dense_3[0][0]',                \n",
      "                                                                  'dense_4[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 98,431\n",
      "Trainable params: 98,431\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "variational_encoder, inputs, codings_mean, codings_log_var = encoder(i=IMG_SIZE, c=CODINGS_SIZE)\n",
    "variational_encoder.summary() # keep the trainable params below 20,000 is advised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f6864ad-45c1-4ecc-9443-af3c811a7744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 16)]              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3072)              52224     \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 32, 32, 3)         0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 32, 32, 8)         32        \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 32, 32, 5)         45        \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 32, 32, 3)         18        \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 32, 32, 3)         12        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 52,331\n",
      "Trainable params: 52,331\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "variational_decoder = decoder(i=IMG_SIZE, c=CODINGS_SIZE)\n",
    "variational_decoder.summary() # keep the trainable params below 20,000 is advised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9eeff647-f148-4f6e-ae4f-3c068f4854dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, codings = variational_encoder(inputs)\n",
    "reconstructions = variational_decoder(codings)\n",
    "variational_ae = keras.models.Model(inputs=[inputs], outputs=[reconstructions])\n",
    "latent_loss = -0.5 * K.sum(1 + codings_log_var - K.exp(codings_log_var) - K.square(codings_mean), axis=-1)\n",
    "variational_ae.add_loss(K.mean(latent_loss) / (math.prod(IMG_SIZE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a965caa3-c962-42a1-939a-e2b114b4d408",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d168422-9e87-4910-acff-3d71fb637e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: define your loss function, optimizer, and metric\n",
    "mse = keras.losses.MSE\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    mse_loss = K.mean(mse(x, x_decoded_mean), axis=(1,2)) * 32 * 32 * 3\n",
    "    kl_loss = K.mean(-0.5 * K.sum(1 + codings_log_var - K.exp(codings_log_var) - K.square(codings_mean), axis=-1))\n",
    "    return mse_loss + kl_loss\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def compute_loss(data, reconstruction, alpha = 1):\n",
    "    \n",
    "    # Reconstruction loss-\n",
    "    # recon_loss = tf.keras.losses.mean_squared_error(K.flatten(data), K.flatten(reconstruction))\n",
    "\n",
    "    recon_loss = tf.reduce_mean(\n",
    "        tf.reduce_sum(\n",
    "            # tf.keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
    "            tf.keras.losses.mean_squared_error(data, reconstruction),\n",
    "            axis = (1, 2)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # KL-divergence loss-    \n",
    "    kl_loss = -0.5 * (1 + codings_log_var - tf.square(codings_mean) - tf.exp(codings_log_var))\n",
    "    kl_loss = tf.reduce_mean(\n",
    "        tf.reduce_sum(\n",
    "            kl_loss,\n",
    "            axis = 1\n",
    "        )\n",
    "    )\n",
    "\n",
    "    total_loss = (recon_loss * alpha) + kl_loss\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd70fb74-c075-42a2-988f-f60e1a7b2654",
   "metadata": {},
   "outputs": [],
   "source": [
    "variational_ae.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss=compute_loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e60108-558f-4fa3-8ee7-e7e8d988009f",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "* <b>Bechmark:</b> Validation accuracy fell at `91.48%` within 10 epochs.\n",
    "* If you encounter this `WARNING:tensorflow:AutoGraph ...` in the first epoch, please ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b8b826-6400-4829-a0c3-25c40991f668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "44960/45000 [============================>.] - ETA: 0s - loss: 79.3387 - accuracy: 0.4184"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\\Work\\Devs\\ds-case\\venv\\lib\\site-packages\\keras\\engine\\training_v1.py:2045: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 26s 572us/sample - loss: 79.3060 - accuracy: 0.4185 - val_loss: 45.1897 - val_accuracy: 0.5448\n",
      "Epoch 2/10\n",
      "45000/45000 [==============================] - 24s 544us/sample - loss: 42.3670 - accuracy: 0.5442 - val_loss: 41.1318 - val_accuracy: 0.5521\n",
      "Epoch 3/10\n",
      "45000/45000 [==============================] - 30s 658us/sample - loss: 40.4713 - accuracy: 0.5481 - val_loss: 40.1133 - val_accuracy: 0.5533\n",
      "Epoch 4/10\n",
      " 7488/45000 [===>..........................] - ETA: 21s - loss: 39.7890 - accuracy: 0.5539"
     ]
    }
   ],
   "source": [
    "history = variational_ae.fit(X_train, X_train, epochs=EPOCH, batch_size=BATCH_SIZE, validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2360bd97-14c8-483f-b517-4b2ede606767",
   "metadata": {},
   "source": [
    "### Visualize training and validation results\n",
    "\n",
    "Example output:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d932e79d-5bbe-4c9a-bd46-e0d9751e1623",
   "metadata": {},
   "source": [
    "![VAE_acc_loss](img/vae_plot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5942978-05b1-44fa-8829-fd235fe9813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: plot the loss and accuracy results from both training and validation, as depicted in the image above\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09893863-ac76-4456-b07a-00c54225d6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e079105-bee9-4c22-914f-50e86ddd5701",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ad17ee-fbd6-4dfa-8772-b2821000baa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_reconstructions(variational_ae, X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf80284-3dd9-4f00-a106-fdac434fb589",
   "metadata": {},
   "source": [
    "### Copyright 2022 PT. Agriaku Digital Indonesia\n",
    "* You may NOT use this file except there is written permission from AgriAku.\n",
    "* Any questions can be address to `nicholas.dominic@agriaku.com`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
